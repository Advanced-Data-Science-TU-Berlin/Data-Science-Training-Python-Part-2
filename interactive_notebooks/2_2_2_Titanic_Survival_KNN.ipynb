{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkNPx1Ew0fCfgVB+rJjHVL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Advanced-Data-Science-TU-Berlin/Data-Science-Training-Python-Part-2/blob/main/interactive_notebooks/2_2_2_Titanic_Survival_KNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Titanic: Machine Learning from Disaster\n",
        "![picture](https://creazilla-store.fra1.digitaloceanspaces.com/cliparts/1722941/titanic-clipart-md.png)\n",
        "\n",
        "The Titanic dataset is a well-known dataset in the field of data science and machine learning. It contains information about passengers on the Titanic, including whether they survived or not. Here are the details about the columns in the dataset:\n",
        "\n",
        "- PassengerId: A unique identifier for each passenger.\n",
        "- Survived: Indicates whether the passenger survived (1) or not (0).\n",
        "- Pclass: Ticket class (1st, 2nd, or 3rd).\n",
        "- Name: Passenger's name.\n",
        "- Sex: Passenger's gender (male or female).\n",
        "- Age: Passenger's age.\n",
        "- SibSp: Number of siblings/spouses aboard.\n",
        "- Parch: Number of parents/children aboard.\n",
        "- Ticket: Ticket number.\n",
        "- Fare: Passenger fare.\n",
        "- Cabin: Cabin number.\n",
        "- Embarked: Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton).\n",
        "\n",
        "The primary goal of working with this dataset is often to predict whether a passenger survived based on other features. It's a binary classification problem, where 'Survived' is the target variable.\n",
        "\n",
        "## Insights and Considerations:\n",
        "- Missing Values: The dataset may have missing values, especially in columns like 'Age,' 'Cabin,' and 'Embarked.' Handling missing values is an important part of preprocessing.\n",
        "\n",
        "- Categorical Features: Features like 'Sex' and 'Embarked' are categorical. These need to be converted to numerical values for machine learning algorithms.\n",
        "\n",
        "- Feature Engineering: Creating new features or modifying existing ones can enhance the predictive power of the model. For example, creating a 'FamilySize' feature by combining 'SibSp' and 'Parch' might be useful.\n",
        "\n",
        "- Exploratory Data Analysis (EDA): EDA helps in understanding the distribution of data, identifying patterns, and making informed decisions during preprocessing.\n"
      ],
      "metadata": {
        "id": "-BnpzbTiZL1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "exM225IJI3A2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Titanic dataset\n",
        "# Hint: use pd.read_csv and pass the url\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "titanic_df = <your-code-here>\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "titanic_df.head()"
      ],
      "metadata": {
        "id": "GihlIy1lZQjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "CMV2LcD7ZWXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore the dataset\n",
        "\n",
        "# Basic Information\n",
        "# Hint: use titanic_df.info()\n",
        "print(<your-code-here>)\n",
        "\n",
        "# Summary Statistics\n",
        "# Hint: use titanic_df.describe()\n",
        "print(<your-code-here>)\n",
        "\n",
        "# Check for missing values\n",
        "# Hint: use titanic_df.isnull().sum()\n",
        "print(<your-code-here>)"
      ],
      "metadata": {
        "id": "THlVb8DRZQ-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Target Variable"
      ],
      "metadata": {
        "id": "wXnfF1NpJfCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the distribution of the target variable 'Survived'\n",
        "# Hint: use 'Survived' as the input of countplot\n",
        "sns.countplot(x=<target-variable>, data=titanic_df)\n",
        "\n",
        "plt.title('Distribution of Survived (1: Survived, 0: Not Survived)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F8fdWaIRZpqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Point-wise Correlation Pairplot\n",
        "\n",
        "# Select numerical columns for correlation pairplot\n",
        "numerical_cols = titanic_df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Create a point-wise correlation pairplot\n",
        "# Hint: use titanic_df[numerical_cols] as the input for pairplot and 'Survived' for hue\n",
        "sns.pairplot(<df-with-numerical-columns>, hue=<target-variable>, height=1.8,\n",
        "                  aspect=1.8, plot_kws=dict(edgecolor=\"k\",\n",
        "                  linewidth=0.5), diag_kind=\"kde\")\n",
        "plt.suptitle(\"Point-wise Correlation Pairplot\", y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z7gwOVgUZZwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values\n",
        "\n",
        "# Drop 'Cabin' column due to high missing values\n",
        "# Hint: use titanic_df.drop and pass 'Cabin' and axis=1 as inputs\n",
        "titanic_df = titanic_df.drop('Cabin', axis=1)\n",
        "\n",
        "# Fill missing values in 'Age' with the median\n",
        "# Hint: use titanic_df['Age'].median() inside fillna\n",
        "titanic_df['Age'].fillna(<age-median>, inplace=True)\n",
        "\n",
        "# Fill missing values in 'Embarked' with the mode\n",
        "# Hint: use titanic_df['Embarked'].mode()[0] inside fillna\n",
        "titanic_df['Embarked'].fillna(<embarked-mode>, inplace=True)"
      ],
      "metadata": {
        "id": "VnDhgn0zZyzL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle categorical features\n",
        "\n",
        "# Convert 'Sex' to numerical values (0 for Female, 1 for Male)\n",
        "# Hint: use {'female': 0, 'male': 1} for the map\n",
        "titanic_df['Sex'] = titanic_df['Sex'].map(<gender-mapping-dict>)\n",
        "\n",
        "# Convert 'Embarked' to numerical values using one-hot encoding\n",
        "titanic_df = pd.get_dummies(titanic_df, columns=['Embarked'], drop_first=True)"
      ],
      "metadata": {
        "id": "rEXCyxvneCXm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Selection"
      ],
      "metadata": {
        "id": "fu10m4r4fEng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select relevant features for classification\n",
        "X = titanic_df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']]\n",
        "y = titanic_df['Survived']"
      ],
      "metadata": {
        "id": "jnw_Tpl8eE7c"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train/Test Split\n",
        "Let's split the data into train and test sets. In sklearn.model_selection package there is a function to do so which makes our lives easier. Remember you can also split data manually."
      ],
      "metadata": {
        "id": "fvFGKXqQe8BY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "# Hint: use train_test_split and pass X, y, test_size=0.2, random_state=42\n",
        "X_train, X_test, y_train, y_test = <your-code-here>"
      ],
      "metadata": {
        "id": "FVpYy141eHhL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Scaling\n",
        "\n",
        "Feature scaling is crucial in K-Nearest Neighbors (KNN) and many other machine learning algorithms because it helps ensure that all features contribute equally to the distance computations. In the case of KNN, the algorithm classifies a data point by considering the majority class among its k nearest neighbors. The \"distance\" between data points is typically measured using metrics like Euclidean distance.\n",
        "\n",
        "Here's why feature scaling is important in the context of KNN:\n",
        "\n",
        "- Distance Computation: KNN relies on the concept of proximity or distance between data points. Features with larger scales or ranges may dominate the distance computation compared to features with smaller scales. This can lead to biased predictions.\n",
        "\n",
        "- Uniform Contribution: Scaling features to a similar range ensures that each feature contributes more uniformly to the distance metric. Without scaling, a feature with a larger scale could overshadow the influence of other features.\n",
        "\n",
        "- Equal Importance: Scaling is essential when the features are measured in different units or have different magnitudes. For example, the 'Age' feature might be measured in years, while the 'Fare' feature might be measured in dollars. Scaling makes each feature contribute proportionally to the overall distance, ensuring that no single feature dominates the calculation.\n",
        "\n",
        "- Faster Convergence: Feature scaling can help the KNN algorithm converge faster during training. The optimization process is often more efficient when the features are on a similar scale."
      ],
      "metadata": {
        "id": "1iP-zPPTeLsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform training data\n",
        "# Hint: use scaler.fit_transform on X_train\n",
        "X_train_scaled = <your-code-here>\n",
        "\n",
        "# Transform test data\n",
        "# Hint: use scaler.transform on X_test\n",
        "X_test_scaled = <your-code-here>"
      ],
      "metadata": {
        "id": "bQMSWaBVeJLG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building and Training a K-Nearest Neighbors (KNN) Classifier"
      ],
      "metadata": {
        "id": "VahWg7r0e1dK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and train the KNN Classifier\n",
        "# Hint: use KNeighborsClassifier and pass n_neighbors=5 as input\n",
        "knn_classifier = <your-code-here>\n",
        "\n",
        "# Fit the knn classifier on training data\n",
        "# Hint: use knn_classifier.fit and pass X_train_scaled and y_train as input\n",
        "knn_classifier.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "bVGEvg4uedSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the Algorithm\n",
        "For evaluating an algorithm, confusion matrix, precision, recall and f1 score are the most commonly used metrics. The confusion_matrix and classification_report methods of the sklearn.metrics can be used to calculate these metrics. Take a look at the following script:"
      ],
      "metadata": {
        "id": "yE3qzkPHfL08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "# Hint: use knn_classifier.predict and pass X_test_scaled\n",
        "y_pred = <your-code-here>\n",
        "\n",
        "# Evaluate the model\n",
        "# Calculate acuuracy score\n",
        "# Hint: use accuracy_score and pass y_test, y_pred\n",
        "accuracy = <your-code-here>\n",
        "\n",
        "# Calculate confusion matrix\n",
        "# Hint: use confusion_matrix and pass y_test, y_pred\n",
        "conf_matrix = <your-code-here>\n",
        "\n",
        "# Create classification report\n",
        "# Hint: use classification_report and apply y_test, y_pred\n",
        "classification_rep = <your-code-here>\n",
        "\n",
        "# Display evaluation metrics\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_rep)"
      ],
      "metadata": {
        "id": "8gVD_A11fKyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HTXBSuSrfdWG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}