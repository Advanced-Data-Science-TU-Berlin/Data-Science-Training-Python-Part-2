{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_3_text_classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNAlXVk+xgq1sqqfumKpLaz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Advanced-Data-Science-TU-Berlin/Data-Science-Training-Python-Part-2/blob/main/interactive_notebooks/1_1_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Classification\n",
        "Text classification also known as text tagging or text categorization is the process of categorizing text into organized groups. By using Natural Language Processing (NLP), text classifiers can automatically analyze text and then assign a set of pre-defined tags or categories based on its content.\n",
        "\n",
        "In this exercise we are using the data from one of the Kaggle competitions named [“Toxic Comment Classification Challenge”](https://www.kaggle.com/datasets/julian3833/jigsaw-toxic-comment-classification-challenge). In this competition, we’re challenged to build a multi-headed model that’s capable of detecting different types of toxicity like threats, obscenity, insults, and identity-based hate. The dataset contains comments from Wikipedia’s talk page edits. (So, along with text classification we will also be learning how to implement multi-output/multi-label classification)\n",
        "\n",
        "> The dataset for this competition contains text that may be considered profane, vulgar, or offensive. We do not encourage such words and this is only for experiment purposes.\n",
        "\n",
        "Let's start with loading the data and look at its structure:\n",
        "\n"
      ],
      "metadata": {
        "id": "6VL-6XAULLhY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A7kMIMULWhad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading datasets from Kaggle\n",
        "\n",
        "### Check for Kaggle API token\n",
        "\n",
        "Do you know how to download datasets from Kaggle using the Kaggle API?\n",
        "Do you have your Kaggle API token set up?"
      ],
      "metadata": {
        "id": "T0FyvU0r1m5k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0U2A-CgKxvH",
        "outputId": "347164d2-0af7-40b9-c1e0-36fac4012bff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.10/dist-packages (0.1.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.5.16)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.31.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.6)\n"
          ]
        }
      ],
      "source": [
        "# opendatasets is a Python library for downloading datasets from online sources\n",
        "# like Kaggle and Google Drive using a simple Python command.\n",
        "# install opendatasets python libary\n",
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "od.download(\"https://www.kaggle.com/datasets/julian3833/jigsaw-toxic-comment-classification-challenge\", force=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER66kHEfL6V-",
        "outputId": "1d399a4c-1897-4e65-b9e8-b12ec7a1de5c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: mahnaznmz\n",
            "Your Kaggle Key: ··········\n",
            "Downloading jigsaw-toxic-comment-classification-challenge.zip to ./jigsaw-toxic-comment-classification-challenge\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 53.4M/53.4M [00:00<00:00, 78.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "% matplotlib inline\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12_b7SwX2T9T",
        "outputId": "73faacc3-8cc7-47d9-bf67-9952a4238158"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%` not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's read data related to train, test and test labels into Pandas Dataframes\n",
        "# Hint: Use the 'pd.read_csv()' function to read the data from the file\n",
        "# The files are stored in /content/jigsaw-toxic-comment-classification-challenge/\n",
        "train_df = <your code here>\n",
        "test_df = <your code here>\n",
        "test_labels = <your code here>\n",
        "\n",
        "# Display a few lines of the training data\n",
        "<write your code>"
      ],
      "metadata": {
        "id": "1ZQimasfMZzV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Target Variables\n",
        "From the data bove what are the target variables?"
      ],
      "metadata": {
        "id": "xli_S5YK647v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list with the name of target variables:\n",
        "# target_variables = [<trarget-variable-names-here>]"
      ],
      "metadata": {
        "id": "Hz0u4JEn626a"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at some statistics from the data:"
      ],
      "metadata": {
        "id": "wu-SbcRpPJjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the size of the train and test sets\n",
        "# Hint: Use the 'len()' function to get the size of the DataFrames\n",
        "print(\"Train Size:\\t\", <your code here>)\n",
        "print(\"Test Size:\\t\", <your code here>)"
      ],
      "metadata": {
        "id": "NF-Z5XjFPNhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(train_df)\n",
        "# Display the probability distribution of target variables in the training set\n",
        "# Note: apply pd.Series.value_counts function\n",
        "# value_counts args: [normalize, sort, ascending, bins, dropna]\n",
        "# Consider: normalize=True, sort=True, ascending=False, bins=None, dropna=False\n",
        "display(\n",
        "    train_df[<target-variables-here>]\n",
        "        .apply(<function-here>, args = (<comma-seperated-arg-values-here>))\n",
        "        )"
      ],
      "metadata": {
        "id": "ZNosdlPM6DIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see we have a class imbalance in almost all the training target variables.\n",
        "\n",
        "Here we also have multiple independent variables.\n",
        "\n",
        "## Split the dataset for model evaluation\n",
        "Let's split train dataset into training and validation sets:"
      ],
      "metadata": {
        "id": "NTdlOh6RQ1KN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the dependent variable\n",
        "# Hint: Use the column 'comment_text' as the dependent variable\n",
        "X = train_df[<dependent-column-name>]\n",
        "\n",
        "# Define the independent variables\n",
        "# Hint: Use the target value columns\n",
        "y = train_df[<target-variables-here>]\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "# Hint: Use 'train_test_split()' with shuffle=True and any random_state=42\n",
        "X_train, X_val, y_train, y_val = <your-code-here>"
      ],
      "metadata": {
        "id": "kGIqeX6a9K1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing\n",
        "Preprocessing is one of the vital steps in NLP like any other ML task. In NLP, it helps to get rid of unhelpful parts of the data, or noise, by converting all characters to lowercase, removing punctuation marks, and removing stop words and typos. In this case, `punctuations` and `numbers` are removed along with `stopwords` like in, the, of so that these can be removed from texts as these words don't help in determining the classes (Whether a sentence is toxic or not)\n",
        "\n",
        "> In this exercise we are also using NLTK which is a leading platform for building Python programs to work with human language data. To ream more check [here](https://www.nltk.org/).\n",
        "\n",
        "\n",
        "Let's create a preprocessing function we can then pass it to our CountVectorizer model."
      ],
      "metadata": {
        "id": "WlvC_J0NLLfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "nltk.download('stopwords') # Download the stop-words\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Get the English stopwords\n",
        "en_stopwords = set(stopwords.words('english'))\n",
        "print(\"EN Stopwords:\", en_stopwords)\n",
        "\n",
        "# Function for basic cleaning/preprocessing texts\n",
        "# Hint: The 'clean' function should take a document ('doc') and a set of stopwords as parameters\n",
        "# It should remove punctuation marks, numbers, and stopwords, and convert the text to lowercase\n",
        "def clean(<set-inputs-here>):\n",
        "    # Removal of punctuation marks (.,/\\][{} etc) and numbers\n",
        "    doc = \"\".join([char for char in doc if char not in string.punctuation and not char.isdigit()])\n",
        "    # Removal of stopwords\n",
        "    doc = \" \".join([token for token in doc.split() if token not in stop_words])\n",
        "    # Convert the text into lowercase using doc.lower()\n",
        "    doc = <your-code-here>\n",
        "    return doc"
      ],
      "metadata": {
        "id": "80Qsr6R0_pgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag Of Words\n",
        "The Bag of Words (BoW) model is a common technique used in natural language processing and text mining to represent text data as numerical features. The basic idea is to convert a collection of text documents into a matrix of word occurrences or frequencies.\n",
        "\n",
        "Here's a brief explanation of the Bag of Words model:\n",
        "\n",
        "1) Tokenization:\n",
        "The first step is to break down each document or sentence into individual words, known as tokens.\n",
        "\n",
        "2) Vocabulary Building:\n",
        " Create a vocabulary, which is a unique set of all the words present in the entire collection of documents. Each word in the vocabulary is assigned a unique index.\n",
        "\n",
        "3) Vectorization:\n",
        "Represent each document as a vector, where each element of the vector corresponds to the count or frequency of a word from the vocabulary. The length of the vector is equal to the size of the vocabulary.\n",
        "\n",
        "4) Sparse Representation:\n",
        "Since most documents use only a small subset of the entire vocabulary, the resulting vectors are often sparse (mostly filled with zeros), making the representation efficient.\n",
        "\n",
        "The Bag of Words model ignores the order and structure of words in a document but captures the occurrence or frequency of each word. It's a fundamental method used in various natural language processing tasks, such as text classification, sentiment analysis, and document clustering. However, it doesn't consider the semantic relationships between words. Advanced models like TF-IDF and word embeddings address some limitations of the basic Bag of Words approach.\n",
        "\n",
        "\n",
        "Now let's create a bag of words model with a maximum of 5000 most-frequent words (as including all the words will make the dataset sparse and will only add noise).Also, Clean the dataset when creating the dataset using bag of words"
      ],
      "metadata": {
        "id": "w-nPFkl0STvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Create a CountVectorizer instance\n",
        "# - 'max_features': Limits the number of features (words) to the top 5000 most frequent ones.\n",
        "# - 'preprocessor': A function to clean and preprocess the text data before vectorization.\n",
        "vectorizer = <your-code-here>\n",
        "\n",
        "# Transform the training data into a document-term matrix (DTM)\n",
        "# - 'fit_transform': Fits the vectorizer to the training data and transforms it into a DTM.\n",
        "# Note: use fit_transform on vectorizer and pass the X_train as input\n",
        "X_train_dtm = <your-code-here>\n",
        "\n",
        "# Transform the validation data into a document-term matrix (DTM)\n",
        "# - 'transform': Uses the same vectorizer to transform the validation data into a DTM.\n",
        "# Note: use transform on vectorizer and pass the X_val as input\n",
        "X_val_dtm = <your-code-here>\n",
        "\n",
        "# Display the shape of the resulting document-term matrices\n",
        "print(X_train_dtm.shape, X_val_dtm.shape)"
      ],
      "metadata": {
        "id": "JWPggtZzAuuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see the same number of rows in train and validation datasets are 5000 columns which are essentially number of occurences of the 5000 most common words in each sentence.\n",
        "Let's look at the 5 samples of bag of words vector:"
      ],
      "metadata": {
        "id": "cLHRoBGiShBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(X_train_dtm.A[:5], columns = vectorizer.get_feature_names())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "aOtgOMhCNp-Q",
        "outputId": "8519650d-a9ce-42bb-895d-1e306f5b104f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   abc  abide  ability  able  abortion  about  absence  absolute  absolutely  \\\n",
              "0    0      0        0     0         0      0        0         0           0   \n",
              "1    0      0        0     0         0      0        0         0           0   \n",
              "2    0      0        0     0         0      0        0         0           0   \n",
              "3    0      0        0     0         0      0        0         0           0   \n",
              "4    0      0        0     0         0      0        0         0           0   \n",
              "\n",
              "   absurd  ...  yourselfgo  youth  youtube  youve  ytmndin  yugoslavia  \\\n",
              "0       0  ...           0      0        0      0        0           0   \n",
              "1       0  ...           0      0        0      0        0           0   \n",
              "2       0  ...           0      0        0      0        0           0   \n",
              "3       0  ...           0      0        0      0        0           0   \n",
              "4       0  ...           0      0        0      0        0           0   \n",
              "\n",
              "   zealand  zero  zionist  zone  \n",
              "0        0     0        0     0  \n",
              "1        0     0        0     0  \n",
              "2        0     0        0     0  \n",
              "3        0     0        0     0  \n",
              "4        0     0        0     0  \n",
              "\n",
              "[5 rows x 5000 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-00fd1a2a-571c-4ec7-87e3-5951b8382ea3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abc</th>\n",
              "      <th>abide</th>\n",
              "      <th>ability</th>\n",
              "      <th>able</th>\n",
              "      <th>abortion</th>\n",
              "      <th>about</th>\n",
              "      <th>absence</th>\n",
              "      <th>absolute</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>absurd</th>\n",
              "      <th>...</th>\n",
              "      <th>yourselfgo</th>\n",
              "      <th>youth</th>\n",
              "      <th>youtube</th>\n",
              "      <th>youve</th>\n",
              "      <th>ytmndin</th>\n",
              "      <th>yugoslavia</th>\n",
              "      <th>zealand</th>\n",
              "      <th>zero</th>\n",
              "      <th>zionist</th>\n",
              "      <th>zone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 5000 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-00fd1a2a-571c-4ec7-87e3-5951b8382ea3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-00fd1a2a-571c-4ec7-87e3-5951b8382ea3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-00fd1a2a-571c-4ec7-87e3-5951b8382ea3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, in this vector whenever the word is present in the comment it will be >1 otherwise 0 (showing the number of occurance).\n",
        "\n",
        "The Bag of words is pretty much sparse (we can further reduce the max_features if required). This will be the input for a Machine Learning Classifier."
      ],
      "metadata": {
        "id": "550XjCMVTBjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Output Classification\n",
        "Since we need to classify each sentence as toxic or not, severe_toxic or not, obscene or not, threat or not, insult or not, and identity_hate or not, we need to classify the sentence against 6 output variables (This is called Multi-Label Classification which is different from multi-class classification where a target variable has more than 2 options e.g. a sentence can be positive, negative and neutral)\n",
        "\n",
        "We will be using MultiOutputClassifier from sklearn which as mentioned earlier is a wrapper. This strategy consists of fitting one classifier per target.\n"
      ],
      "metadata": {
        "id": "T1u7cAtxTZsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to use scikit-learn to train two different multi-output classification models:\n",
        "\n",
        "1) a Naive Bayes model and\n",
        "\n",
        "2) a Logistic Regression model.\n",
        "\n",
        "### Naive Bayes Model\n",
        "Naive Bayes is a family of probabilistic classification algorithms based on Bayes' theorem with the \"naive\" assumption of independence between features. Despite its simplicity, it's a powerful and efficient method, especially for text classification tasks.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:600/1*aFhOj7TdBIZir4keHMgHOw.png\">\n",
        "\n",
        "### Logistic Regression (logistic/Logit Model)\n",
        "A Logistic Regression, often referred to as Logit Model, is a statistical method used for binary classification problems with the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick.\n",
        "\n",
        "This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc. Each object being detected in the image would be assigned a probability between 0 and 1, with a sum of one.\n",
        "\n",
        "<img src=\"https://www.saedsayad.com/images/LogReg_1.png\">\n",
        "\n",
        "Here's a breakdown of the code:"
      ],
      "metadata": {
        "id": "5d6U3BrcC0BZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "cuc8n4ElD54w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes Model\n",
        "# - MultiOutputClassifier: Extends a classifier to handle multi-output problems.\n",
        "# - MultinomialNB: Naive Bayes classifier suitable for classification with discrete features.\n",
        "nb_model = MultiOutputClassifier(MultinomialNB())\n",
        "# fit the naive base model on train data\n",
        "# Hint: use fit function on nb_model and pass X_train_dtm and y_train as inputs\n",
        "<your-code-here>"
      ],
      "metadata": {
        "id": "to_ZJqEIDLu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression Model\n",
        "# - MultiOutputClassifier: Extends a classifier to handle multi-output problems.\n",
        "# - LogisticRegression: Logistic Regression classifier.\n",
        "#   - 'class_weight': 'balanced' penalizes mistakes in samples of each class with the inverse of counts of that class.\n",
        "#   - 'max_iter': Maximum number of iterations for optimization.\n",
        "lr_model = MultiOutputClassifier(LogisticRegression(class_weight='balanced', max_iter=3000))\n",
        "# fit the lr base model on train data\n",
        "# Hint: use fit function on nb_model and pass X_train_dtm and y_train as inputs\n",
        "<your-code-here>"
      ],
      "metadata": {
        "id": "2bTbrWZkDl8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Measuring Performance\n",
        "Let's see what metric we should use here to measure the performance of our models.\n",
        "\n",
        "### ROC - AUC\n",
        "ROC (Receiver Operating Characteristic) Curve tells us about how good the model can distinguish between two things (e.g If a comment is toxic or not). Better models can accurately distinguish between the two. Whereas, a poor model will have difficulties in distinguishing between the two.\n",
        "\n",
        "The ROC curve is a valuable metric for evaluating classification models, especially in scenarios where there is an imbalance in the class distribution.\n",
        "\n",
        "<img src=\"https://www.statology.org/wp-content/uploads/2021/08/read_roc2.png\" width=\"500\">\n",
        "\n",
        "Let's see how does the ROC curves look like for the `toxic` label for both of our trained models:"
      ],
      "metadata": {
        "id": "Xctmfgh-UYK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# A helper function for plotting ROC curve for a model and a given label\n",
        "def plot_roc_auc(model, model_name, label_name, ax):\n",
        "    # Get the index of the label in the columns\n",
        "    label_id = list(y_val.columns).index(label_name)\n",
        "\n",
        "    # Get true labels and predicted probabilities for the positive class\n",
        "    y_vals = y_val[label_name].to_numpy()\n",
        "    y_pred_proba = model.predict_proba(X_val_dtm)[label_id][:, 1]\n",
        "\n",
        "    # Calculate the ROC curve\n",
        "    # Note: use roc_curve function with y_vals and y_pred_proba as input\n",
        "    fpr, tpr, thresholds = <your-code-here>\n",
        "\n",
        "    # Calculate AUC\n",
        "    # Note: use auc function and pass fpr and tpr\n",
        "    auc_value = <your-code-here>\n",
        "\n",
        "    # Plot ROC curve\n",
        "    ax.plot([0, 1], [0, 1], 'k--')  # Diagonal line for reference\n",
        "    ax.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_value:.3f})')\n",
        "\n",
        "    # Set plot labels and title\n",
        "    ax.set_xlabel('False Positive Rate (fpr)')\n",
        "    ax.set_ylabel('True Positive Rate (tpr)')\n",
        "    ax.set_title(f'{model_name} ROC curve for `{label_name}` Label')\n",
        "    ax.legend()\n",
        "\n",
        "# Create subplots for two models\n",
        "fig, ax = plt.subplots(1, 2, figsize=(20, 6))\n",
        "\n",
        "# Plot ROC curve for Naive Bayes model on the 'toxic' label\n",
        "plot_roc_auc(<nb-model>, 'Naive Bayes', 'toxic', ax[0])\n",
        "\n",
        "# Plot ROC curve for Logistic Regression model on the 'toxic' label\n",
        "plot_roc_auc(<lr-model>, 'Logistic Regression', 'toxic', ax[1])"
      ],
      "metadata": {
        "id": "ApM1HfRAGNWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Let's compare the mean ROC-AUC across both models we have trained as the aggregated measurement of their performances. We will be using the predict_proba function of models instead of predict which gives us the probability scores instead of predicted value based on a threshold of 0.5, as it is used by the roc_auc_measure.\n",
        "\n",
        "Let's write a function for calculating the roc_auc:"
      ],
      "metadata": {
        "id": "ecDS8JedHDVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Function for calculating ROC-AUC scores\n",
        "def calculate_roc_auc(y_test, y_pred):\n",
        "    aucs = []\n",
        "\n",
        "    # Calculate the ROC-AUC for each of the target columns\n",
        "    for col in range(y_test.shape[1]):\n",
        "        aucs.append(roc_auc_score(y_test[:, col], y_pred[:, col]))\n",
        "\n",
        "    return aucs"
      ],
      "metadata": {
        "id": "lmSDIfADUI3z"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the performance metrics let’s run the models on the validation dataset"
      ],
      "metadata": {
        "id": "L2L-kQM6U7VI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import mean\n",
        "\n",
        "# Creating an empty list of results\n",
        "results = []\n",
        "\n",
        "# Making predictions from all the trained models and measuring performance for each\n",
        "for model in [nb_model, lr_model]:\n",
        "    # Extracting the name of the model\n",
        "    est = type(model.estimator).__name__\n",
        "\n",
        "    # Actual output variables\n",
        "    y_vals = y_val.to_numpy()\n",
        "\n",
        "    # Model Probabilities for class 1 of each of the target variables\n",
        "    y_preds = np.transpose(np.array(model.predict_proba(X_val_dtm))[:,:,1])\n",
        "\n",
        "    # Calculate Mean of the ROC-AUC\n",
        "    # Note: use mean function on calculate_roc_auc between y_vals and y_preds\n",
        "    mean_auc = <your-code-here>\n",
        "\n",
        "    # Append the name of the model and the mean_roc_auc into the results list\n",
        "    results.append([est, mean_auc])\n",
        "\n",
        "# Output the results as a table\n",
        "result_df = pd.DataFrame(results, columns=[\"Model\", \"Mean AUC\"])\n",
        "result_df"
      ],
      "metadata": {
        "id": "via0uPqKJdvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, Both the models perform really good with LR performing slightly better. So, we will use it as the final model to submit the predictions for the test data. Also, these simple models give pretty good results without much of a hassle or technical know-how, that is why they are still used widely."
      ],
      "metadata": {
        "id": "R9WM94A4VSpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicting Target Values for Test Data\n"
      ],
      "metadata": {
        "id": "S4_qNNGwWQUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the test dataset using Count Vectorizer\n",
        "# Note: use transform on vectorizer and pass 'comment_text' column from test_df\n",
        "X_test_dtm = <your-code-here>\n",
        "\n",
        "# Use the Logistic Regression model to output probabilities and take the probability for class 1\n",
        "y_preds = np.transpose(np.array(lr_model.predict_proba(X_test_dtm))[:,:,1])\n",
        "\n",
        "# Add predicted labels to the test data\n",
        "test_pred_labels = test_df.assign(**pd.DataFrame(y_preds, columns=target_variables))\n",
        "\n",
        "# Calculate average ROC-AUC on the test data\n",
        "test_auc_mean = mean(calculate_roc_auc(test_labels[target_variables].replace(-1, 0).to_numpy(), y_preds))\n",
        "print(\"Mean AUC on Test:\", test_auc_mean)"
      ],
      "metadata": {
        "id": "FVpe3VNaJycX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see we are still having a rather good performance on unseen data ~90%"
      ],
      "metadata": {
        "id": "cgivM4GXcESk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Interpretation/ Word Importance\n",
        "This is the most exciting part. Since, we are just using a simple Logistic Regression model, we can directly use the coefficient values of the model to get an understanding of the predictions made. By doing so, which feature is importance or which word makes a sentence toxic. If we would use a complex model, we could go for SHAP or LIME. Also, since we have 6 output variables, we will have 6 feature importances which will be interesting to see"
      ],
      "metadata": {
        "id": "eLmS7YmhcVLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning the feature names to an empty list\n",
        "# Note: use get_feature_names_out() function on vectorizer to get the list of words\n",
        "feat_impts_df = pd.DataFrame({'word': <your-code-here>})\n",
        "\n",
        "# For all the models, save the feature importances in the list.\n",
        "# 'estimators_' gives the internal models used by the multi-output regressor\n",
        "for target_name, clf in zip(target_variables, lr_model.estimators_):\n",
        "    # Note: use coef_ on clf to get the coefficients and then use flatten() to get the list\n",
        "    feat_impts_df[target_name] = <your-code-here>\n",
        "\n",
        "# Displaying the DataFrame\n",
        "feat_impts_df.head()"
      ],
      "metadata": {
        "id": "ThG3YlTlQNAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top Words Determining Toxicity\n",
        "We will look at Top 5 words which determine if the sentence is a toxic-type or not according to the model."
      ],
      "metadata": {
        "id": "TLwQBDnkc38i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create individual feature importance tables and plots\n",
        "def plot_top_words(df, category, ax, topn=5):\n",
        "    top_words = df[[\"word\", category]].sort_values(by=category, ascending=False).head(topn)\n",
        "    sns.barplot(x=category, y=\"word\", ax=ax, data=top_words)\n",
        "    ax.set_title(f\"Top {topn} Words for {category.capitalize()}\")\n",
        "\n",
        "# Creating subplots for each toxic-type category\n",
        "# Note: use subplots on plt with 2 rows and 3 columns as we have 6 target values\n",
        "# use figsize=(15, 5)\n",
        "fig, axes = <your-code-here>\n",
        "\n",
        "# Plotting top 5 words for each toxic-type category\n",
        "for category, ax in zip(target_variables, axes.flatten()):\n",
        "    # Call plot_top_words function and pass feat_impts_df, category and ax\n",
        "    <your-code-here>\n",
        "\n",
        "plt.text(0.5, -0.1, \"Note: The following plots may contain words that are not suitable for all audiences. Viewer discretion is advised.\",\n",
        "         horizontalalignment='center', verticalalignment='center', transform=plt.gcf().transFigure,\n",
        "         bbox=dict(facecolor='red', alpha=0.5))\n",
        "# Setting a common title for the entire subplot\n",
        "plt.suptitle(\"Feature Importance\")\n",
        "\n",
        "# Adjusting layout for better appearance\n",
        "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZI6E8HLgUous"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the models are quite rightly selecting the most important features and it makes complete sense\n",
        "\n",
        "For e.g. for threats - words like kill, shoot, destroy etc are most important\n",
        "\n",
        "for identity hate - words like nigger, nigga, homosexual, faggot\n",
        "\n",
        "most important words for toxic are less extreme than most important words for severe toxic."
      ],
      "metadata": {
        "id": "jTKM5wSidDZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODOs:\n",
        "1. Try TF-IDF instead of CountVectorizer\n",
        "TF-IDF tend to perform better than CountVectorizer in some cases\n",
        "2. Try ensemble models instead of Vanilla ML models\n",
        "Bagging and Boosting models give better results than classic ML techniques in most cases\n",
        "3. Better Text Preprocessing\n",
        "Typo correction, Lemmatization, etc can be done to further improve the model"
      ],
      "metadata": {
        "id": "_8Kuwe8WdvEU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Useful Links:\n",
        "- https://medium.com/analytics-vidhya/text-classification-from-bag-of-words-to-bert-1e628a2dd4c9\n",
        "- https://towardsdatascience.com/text-classification-using-naive-bayes-theory-a-working-example-2ef4b7eb7d5a#:~:text=1.-,Introduction,is%20independent%20of%20each%20other"
      ],
      "metadata": {
        "id": "i3dCcUhBdVAG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CvF2Zz5XdgRm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}