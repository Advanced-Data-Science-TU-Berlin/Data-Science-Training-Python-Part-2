{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_2_decision_tree_classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPmhXnLhZLYxKjKu6IKpNat",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Advanced-Data-Science-TU-Berlin/Data-Science-Training-Python-Part-2/blob/main/interactive_notebooks/1_2_decision_tree_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification and Regression Trees (CART)\n",
        "Classification and Regression Trees (CART) can be translated into a graph or set of rules for predictive classification. They help when logistic regression models cannot provide sufficient decision boundaries to predict the label. In addition, decision tree models are more interpretable as they simulate the human decision-making process. In addition, decision tree regression can capture non-linear relationships, thus allowing for more complex models. CART tries to split data into subsets so that each subset is as pure or homogeneous as possible.\n",
        "> A pure node is one that results in perfect prediction.\n",
        "\n",
        "In this exercise we are using the **Pima Indians Diabetes Dataset** which is applicable in the field of medical sciences. The objective of the dataset is to diagnostically predict `whether or not a patient has diabetes`, based on certain diagnostic measurements included in the dataset.\n",
        "\n",
        "> To read more about this data check [here](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)\n",
        "\n",
        "<img src=\"https://www.researchgate.net/publication/328766758/figure/fig2/AS:689950496403459@1541508425307/Decision-tree-structure-by-using-all-features-and-Pima-Indians-dataset-From-this.png\">\n",
        "\n",
        "Let's try to read the data and check out what the first few rows of this dataset look like:"
      ],
      "metadata": {
        "id": "Be0xzWb4x5f1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVqmCoaQQEqP"
      },
      "outputs": [],
      "source": [
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "# Download the kaggle dataset from https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database\n",
        "# Note: use od.download and pass the url\n",
        "<your-code-here>"
      ],
      "metadata": {
        "id": "OUNcMev82oCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "% matplotlib inline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3xve7wZ2yLB",
        "outputId": "712ffd52-6707-40d5-ad9f-f1107362999d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%` not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the diabetes data from /content/pima-indians-diabetes-database/diabetes.csv\n",
        "# Note: use pd.read_csv\n",
        "df = <your-code-here>\n",
        "# Visualize few lines of the data\n",
        "<your-code-here>"
      ],
      "metadata": {
        "id": "1T0295MwZK6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After loading the data, we understand the structure & variables. Let's determine the target & feature variables (dependent & independent variables respectively)\n",
        "\n"
      ],
      "metadata": {
        "id": "szJb1BNA3ZvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into features and target variable\n",
        "\n",
        "# Select features (X) - Include all columns except the target variable \"Outcome\"\n",
        "X = <your-code-here>\n",
        "\n",
        "# Select the target variable or label (y)\n",
        "y = <your-code-here>\n"
      ],
      "metadata": {
        "id": "nCyXLXgSZ26V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As part of any Machine Learning process we need to make sure that the data is clean before applying any model.\n",
        "> Important Note: Decision Tree (DT) can handle both continuous and numeric variables. But since we are using Scikit Learn, we need to convert categorical values into numerical.\n",
        "\n",
        "Luckily here we don't have any categorical value.\n",
        "\n",
        "> Another Important Note: DT can automatically handle missing values and they are robust to outliers so we don't need to do anything regarding that.\n",
        "\n",
        "Let's just look closer at the data:\n"
      ],
      "metadata": {
        "id": "4k1gkwlJ4JaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display summary statistics of the dataset\n",
        "# Note: use describe() on df\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "wsls12fnC0ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Distribution of the target value"
      ],
      "metadata": {
        "id": "V0YFqM2Sbp1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the distribution of the target value \"Outcome\"\n",
        "# Note: use value_counts() on Outcome column\n",
        "target_frequency = <your-code-here>\n",
        "\n",
        "print(\"#Outcomes:\")\n",
        "display(target_frequency)\n",
        "\n",
        "# Plot a pie chart showing the distribution\n",
        "# Note use plot.pie on target_frequency also use following parameters to look better\n",
        "# autopct='%1.1f%%', startangle=90, colors=['lightgreen', 'lightcoral']\n",
        "<your-code-here>\n"
      ],
      "metadata": {
        "id": "QJHCHgLcc3x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pair-wise Scatter Plots\n",
        "Pair-wise scatter plots are a visualization tool used to explore relationships between pairs of variables in a dataset. Each point on the plot represents a data point, and the position of the point is determined by the values of two variables.\n",
        "\n",
        "Here, the pair-wise scatter plots are created for different pairs of features, with points colored by the target variable \"Outcome\" (indicating whether a patient has diabetes or not). This type of visualization allows for the examination of potential patterns, trends, or separations between different classes of the target variable."
      ],
      "metadata": {
        "id": "i7dw0_Vcc6Vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair-wise Scatter Plots\n",
        "# Put the 'Outcome' which is our target value into the hue parameter\n",
        "pp = sns.pairplot(df, hue=<target-name>, height=1.8,\n",
        "                  aspect=1.8, plot_kws=dict(edgecolor=\"k\",\n",
        "                  linewidth=0.5), diag_kind=\"kde\")\n",
        "\n",
        "fig = pp.fig\n",
        "fig.subplots_adjust(top=0.93, wspace=0.3)\n",
        "t = fig.suptitle('Pairwise Plots', fontsize=14)"
      ],
      "metadata": {
        "id": "wM3eThfBbwmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see out dataset is slightly imbalanced. Usually it can impact the performance of the DT (similar to most of other classifiers). They are techniques to deal with imbalanced classification. You can read more [here](https://machinelearningmastery.com/cost-sensitive-decision-trees-for-imbalanced-classification/)\n",
        "\n",
        "From the pairwise plots we can see that the data is not linearly separable.\n",
        "\n",
        "## Dataset Split\n",
        "Now let’s divide the data into training & testing sets in the ratio of 70:30."
      ],
      "metadata": {
        "id": "pjwLaDYv5puF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset into training set and test set (70% training and 30% test)\n",
        "# Note: use train_test_split and pass X, y, test_size=0.3, random_state=1\n",
        "X_train, X_test, y_train, y_test = <your-code-here>"
      ],
      "metadata": {
        "id": "E5SL1OqAeg0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's perform the decision tree analysis using scikit learn:"
      ],
      "metadata": {
        "id": "HRkcz2LC8Hty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Decision Tree Classifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Create a Decision Tree classifier object\n",
        "# Note: call DecisionTreeClassifier and use criterion='entropy' as input\n",
        "clf = <your-code-here>\n",
        "\n",
        "# Train the Decision Tree Classifier\n",
        "# Note: call fit function on clf and pass X_train and y_train\n",
        "clf = <your-code-here>\n",
        "\n",
        "# Predict the response for the test dataset\n",
        "# Note: call predict on clf and pass X_test\n",
        "y_pred = <your-code-here>\n"
      ],
      "metadata": {
        "id": "Hm1KMulxe6dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's quickly lookt at the accuracy of the trained model on test data:"
      ],
      "metadata": {
        "id": "So4jejqMAZPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "\n",
        "# Print the accuracy of the model on test data\n",
        "# Note: use metrics.accuracy_score and pass y_test and y_pred\n",
        "print(\"Accuracy:\", <your-code-here>)"
      ],
      "metadata": {
        "id": "g2L7fnPCflvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like our decision tree algorithm has an accuracy more than 70%. A value this high is usually considered good."
      ],
      "metadata": {
        "id": "7X8hizAgAnZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Decision Tree\n",
        "Now that we have created a decision tree, let’s see what it looks like when we visualise it. There are multiple ways to see the result of a DT.\n",
        "1. Print Text Representation: Exporting Decision Tree to the text representation can be useful when working on applications whitout user interface or when we want to log information about the model into the text file. You can check details about export_text in the sklearn docs. But it maybe hard to read specially when the tree is big. Let's take a look:"
      ],
      "metadata": {
        "id": "Cmsftws98a2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tree for decision tree models\n",
        "from sklearn import tree\n",
        "\n",
        "# Export text representation of the tree\n",
        "# Note: use export_text function on tree and pass clf\n",
        "text_representation = <your-code-here>\n",
        "\n",
        "# Print Text Representation\n",
        "print(text_representation)"
      ],
      "metadata": {
        "id": "zirk4vk2gFmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Plot Tree with `plot_tree`: The plot_tree method was added to sklearn in version 0.21. It requires matplotlib to be installed. It allows us to easily produce figure of the tree (without intermediate exporting to graphviz) The more information about plot_tree arguments are in the docs. Let's see how it looks like:\n"
      ],
      "metadata": {
        "id": "EbVZ3dEX99zW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure\n",
        "fig = plt.figure(figsize=(25,20))\n",
        "\n",
        "# Plot the tree\n",
        "# Note: Pass the clf classifier as the first argument of the plot_tree\n",
        "_ = tree.plot_tree(<classifier>,\n",
        "                   feature_names=X.columns,\n",
        "                   class_names=y.name,\n",
        "                   filled=True)"
      ],
      "metadata": {
        "id": "1VYqCiApgM0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can save the figure into a .png file."
      ],
      "metadata": {
        "id": "mKx0Y0Ly-nFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the image\n",
        "# Note: use fig.savefig and pass \"decistion_tree.png\" as the input\n",
        "<your-code-here>"
      ],
      "metadata": {
        "id": "pVAtTJpZ-mZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Visualize Decision Tree with `graphviz`: To plot the tree first we need to export it to DOT format with export_graphviz method. Then we can plot it in the notebook or save to the file. This will provide us with a better view to look at bigger trees.\n",
        "> The DOT format is a plain text graph description language that is commonly used for describing graphs and graph structures\n",
        "Let's take a look:\n"
      ],
      "metadata": {
        "id": "Vp0BqmI__AAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import graphviz\n",
        "import graphviz\n",
        "\n",
        "# Export graphviz from the tree\n",
        "# Note: Pass the clf classifier as the first argument of the export_graphviz\n",
        "dot_data = tree.export_graphviz(<classifier>,\n",
        "                                out_file=None,\n",
        "                                feature_names=X.columns,\n",
        "                                class_names=['no', 'yes'],\n",
        "                                filled=True # paint nodes to indicate majority class for classification\n",
        "                                )\n",
        "\n",
        "# Draw graph\n",
        "graph = graphviz.Source(dot_data, format=\"png\")\n",
        "graph"
      ],
      "metadata": {
        "id": "0k2cMpfcgz_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inside each node:\n",
        "- The question that the decision tree asks to split based on the selected feature\n",
        "- Criterion (Ex. gini or entropy): The function to measure the quality of a split\n",
        "- Samples: The number of samples ended up in each node\n",
        "- Value [X,Y]: The list tells you how many samples at the given node fall into each category (here 0 and 1)\n",
        "- Class: shows the prediction a given node will make and it can be determined from the value list"
      ],
      "metadata": {
        "id": "CZlFtgyzwEJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the graphivz figure\n",
        "# Note: use graph.render and pass \"decision_tree_graphivz\" as input\n",
        "<your-code-here>"
      ],
      "metadata": {
        "id": "DUegwjps_RL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> To read more about DT visualization you can check [this](https://mljar.com/blog/visualize-decision-tree/) link"
      ],
      "metadata": {
        "id": "WvAO9WI7_sBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DT Evaluation\n",
        "As you notice, in this extensive decision tree chart, each internal node has a decision rule that splits the data. But are all of these useful/pure?\n",
        "\n",
        "Gini referred to as Gini ratio measures the impurity of the node in a decision tree. One can assume that a node is pure when all of its records belong to the same class. Such nodes are known as the leaf nodes.\n",
        "\n",
        "In our outcome above, the complete decision tree is difficult to interpret due to the complexity of the outcome. Pruning/shortening a tree is essential to ease our understanding of the outcome and optimise it. This optimisation can be done in one of three ways:\n",
        "- **criterion**: optional (default=”gini”) or Choose attribute selection measure.\n",
        "\n",
        "  This parameter allows us to use the attribute selection measure.\n",
        "- **splitter**: string, optional (default=”best”) or Split Strategy\n",
        "\n",
        "  Allows the user to split strategy. You may choose “best” to choose the best - - split or “random” to choose the best random split.\n",
        "- **max_depth**: int or None, optional (default=None) or Maximum Depth of a Tree\n",
        "\n",
        "  This parameter determines the maximum depth of the tree. A higher value of this variable causes overfitting and a lower value causes underfitting.\n",
        "\n",
        ">According to the paper “Theoretical comparison between the Gini Index and Information Gain criteria”, the frequency of agreement/disagreement of the Gini Index and the Information Gain was only 2% of all cases, so for all intents and purposes you can pretty much use either, but the only difference is entropy might be a little slower to compute because it requires you to compute a logarithmic function.\n",
        "\n",
        "> More information regarding DT parameters is [here](https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680)\n",
        "\n"
      ],
      "metadata": {
        "id": "dyPHcws3AD84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation on Pruning in Decision Trees:\n",
        "Pruning is the process of shortening a decision tree to improve its interpretability and optimize its performance.\n",
        "We will vary the maximum depth of the tree as a control variable for pre-pruning.\n",
        "\n",
        "Let’s try max_depth=3."
      ],
      "metadata": {
        "id": "r_bzl6xbh9z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Decision Tree classifier object with pre-pruning parameters\n",
        "# Note: use We use DecisionTreeClassifier with \"entropy\" as the criterion and set max_depth=3 for pre-pruning\n",
        "# This helps in creating a simpler tree for better interpretability\n",
        "clf = <your-code-here>\n",
        "\n",
        "# Train the Decision Tree Classifier\n",
        "clf = <your-code-here>\n",
        "\n",
        "# Predict the response for the test dataset\n",
        "y_pred = <your-code-here>\n",
        "\n",
        "# Model Accuracy: Evaluate the performance of the classifier\n",
        "accuracy = <your-code-here>\n",
        "\n",
        "# Print the accuracy of the model\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "TtFr4QxPiKGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On Pre-pruning, the accuracy of the decision tree algorithm increased to 77.05%, which is clearly better than the previous model.\n",
        "\n",
        "Let's look at the DT:"
      ],
      "metadata": {
        "id": "X1U0xAgvBofY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DOT data\n",
        "dot_data = tree.export_graphviz(clf,\n",
        "                                out_file=None,\n",
        "                                feature_names=X.columns,\n",
        "                                class_names=['No', 'Yes'],\n",
        "                                filled=True)\n",
        "\n",
        "# Draw graph\n",
        "graph = graphviz.Source(dot_data, format=\"png\")\n",
        "graph"
      ],
      "metadata": {
        "id": "RJ5mf4ebBlRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning in DT\n",
        "Hyperparameters are model parameters whose values are set before training. Why should we tune the hyperparameters of a model? Because we don’t really know their optimal values in advance. A model with different hyperparameters is, actually, a different model so it may have a lower performance.If the model has several hyperparameters, we need to find the best combination of values of the hyperparameters searching in a multi-dimensional space. That’s why hyperparameter tuning, which is the process of finding the right values of the hyperparameters, is a very complex and time-expensive task.\n",
        "### Grid Search\n",
        "Grid search is the simplest algorithm for hyperparameter tuning. Basically, we divide the domain of the hyperparameters into a discrete grid. Then, we try every combination of values of this grid, calculating some performance metrics using cross-validation. The point of the grid that maximizes the average value in cross-validation, is the optimal combination of values for the hyperparameters.\n",
        "> To read more check [here](https://www.yourdatateacher.com/2021/05/19/hyperparameter-tuning-grid-search-and-random-search/)\n"
      ],
      "metadata": {
        "id": "15df5TOnDHkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid based on the results of random search\n",
        "# Note: use [2, 3, 5, 10, 20] for max_depth and [\"gini\", \"entropy\"] for criterion\n",
        "params = {\n",
        "    'max_depth': <your-code-here>,\n",
        "    'criterion': <your-code-here>\n",
        "}\n",
        "\n",
        "# Create a Decision Tree classifier with a fixed random_state=1\n",
        "# Note: use DecisionTreeClassifier and pass random_state=1 as input\n",
        "dt = <your-code-here>\n",
        "\n",
        "# Instantiate the grid search model\n",
        "# Note: pass corresponding parameters to GridSearchCV\n",
        "# use cv=4 and scoring=\"accuracy\"\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=<dt-classifier>,\n",
        "    param_grid=<params>,\n",
        "    cv=<number-of-cross-validations>,\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    scoring=<scoring-metric>)\n",
        "\n",
        "# Fit the grid search model on the training data\n",
        "# Note: use grid_search.fit and pass X_train, y_train\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "_MGrC_3EkqEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation on Analyzing Grid Search Results\n",
        "\n",
        "Let's see the results:"
      ],
      "metadata": {
        "id": "ufMFHoXAGF5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with the grid search results\n",
        "# Note: pass grid_search.cv_results_ to pd.DataFrame\n",
        "score_df = <your-code-here>\n",
        "\n",
        "# Display the first few rows of the scores DataFrame\n",
        "print(\"Scores DataFrame: -----------\")\n",
        "display(score_df.head())\n",
        "\n",
        "# Display the top 5 results based on the mean_test_score\n",
        "print(\"Top-5 Scores: -----------\")\n",
        "# Note: use score_df.nlargest and pass 5 and \"mean_test_score\" as inputs\n",
        "display(<your-code-here>)\n",
        "\n",
        "# Extract and display the best estimator (model with the best hyperparameters)\n",
        "# Note: use grid_search.best_estimator_\n",
        "dt_best = <your-code-here>\n",
        "print(\"Best Estimator: -----------\")\n",
        "display(dt_best)"
      ],
      "metadata": {
        "id": "pQTaHzuDmEvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the best parameter let's see its tree and performance but first let's write some helper functions:"
      ],
      "metadata": {
        "id": "NApVsw5RG_qM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(dt_classifier):\n",
        "    # Evaluate the model on the training set\n",
        "    # Note: use dt_classifier.predict and pass X_train\n",
        "    y_pred_train = <your-code-here>\n",
        "    # Note: use metrics.accuracy_score and pass y_train and y_pred_train\n",
        "    train_accuracy = <your-code-here>\n",
        "\n",
        "    # Display the confusion matrix for the training set\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(20,5))\n",
        "    ax[0].set_title('Train Confusion Matrix')\n",
        "    metrics.ConfusionMatrixDisplay.from_estimator(dt_classifier, X_train, y_train, cmap='ocean', ax=ax[0])\n",
        "\n",
        "    print(\"Train Accuracy :\", train_accuracy)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    # Note: use dt_classifier.predict and pass X_test\n",
        "    y_pred_test = <your-code-here>\n",
        "    # Note: use metrics.accuracy_score and pass y_train and y_pred_test\n",
        "    test_accuracy = <your-code-here>\n",
        "\n",
        "    # Display the confusion matrix for the test set\n",
        "    ax[1].set_title(\"Test Confusion Matrix\")\n",
        "    metrics.ConfusionMatrixDisplay.from_estimator(dt_classifier, X_test, y_test, cmap='ocean', ax=ax[1])\n",
        "\n",
        "    print(\"Test Accuracy :\", test_accuracy)\n",
        "\n",
        "def plot_dt_graph(clf):\n",
        "    # Create DOT data for decision tree visualization\n",
        "    dot_data = tree.export_graphviz(clf,\n",
        "                                    out_file=None,\n",
        "                                    feature_names=X.columns,\n",
        "                                    class_names=y.name,\n",
        "                                    filled=True)\n",
        "\n",
        "    # Draw the decision tree graph\n",
        "    graph = graphviz.Source(dot_data, format=\"png\")\n",
        "\n",
        "    return graph"
      ],
      "metadata": {
        "id": "Lz0rvzL-jOXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(dt_best)\n",
        "plot_dt_graph(dt_best)"
      ],
      "metadata": {
        "id": "Gxwq4USYGH-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Useful links:\n",
        "- https://www.kaggle.com/code/gauravduttakiit/hyperparameter-tuning-in-decision-trees/notebook\n",
        "- https://www.kaggle.com/code/mamun18/decision-tree-practice-with-car-evaluation-dataset\n",
        "- https://www.springboard.com/blog/data-science/decision-tree-implementation-in-python/\n",
        "- https://towardsdatascience.com/id3-decision-tree-classifier-from-scratch-in-python-b38ef145fd90"
      ],
      "metadata": {
        "id": "HHUQ0JrkHY6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ggAHhOnYHJZw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}